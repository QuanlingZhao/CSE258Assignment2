{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SlHADAO-FnQ_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPPO2NYqJF8C",
    "outputId": "cee42c8a-03ab-4766-f405-22f9f945619c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 1.13.0 CUDA 11.7\n",
      "Device: cuda:0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, device(type='cuda'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
    "print('Device:', torch.device('cuda:0'))\n",
    "\n",
    "!nvcc --version\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available(), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U7EsH32EFnRA"
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "NUM_TARGETS= 3\n",
    "\n",
    "USER_PATHWAY  = [256, 128, 64]\n",
    "ITEM_PATHWAY = [256, 128, 64]\n",
    "COMBINED_PATHWAY = [256, 128, 64, 16]\n",
    "\n",
    "EMBED_DIM = 40\n",
    "NUM_ITEM_EMBED = 1378\n",
    "NUM_USER_EMBED = 47958\n",
    "NUM_CUPSIZE_EMBED =  12\n",
    "NUM_CATEGORY_EMBED = 7\n",
    "\n",
    "NUM_USER_NUMERIC = 5\n",
    "NUM_ITEM_NUMERIC = 2\n",
    "\n",
    "DROPOUT = 0.3\n",
    "\n",
    "EPOCHS = 2\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ehDAsYrG96RX"
   },
   "outputs": [],
   "source": [
    "class ModCloth(torch.utils.data.Dataset):\n",
    "    def __init__(self,datapath):\n",
    "        self.data = pd.read_csv(datapath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        r = self.data.iloc[idx,:]\n",
    "\n",
    "        return {\n",
    "            \"user_id\" : np.array(r['user_id'], dtype=np.int64),\n",
    "            \"cup_size\" : np.array(r['cup_size'], dtype=np.int64),\n",
    "            \"user_numeric\" : np.array([r['waist'], r['hips'], r['bra_size'], r['height'], r['shoe_size']], dtype=np.float32),\n",
    "            \"item_id\" : np.array(r['item_id'], dtype = np.int64),\n",
    "            \"category\" :np.array(r['category'], dtype = np.int64),\n",
    "            \"item_numeric\" : np.array([r['size'], r['quality']], dtype=np.float32),\n",
    "            \"fit\" : np.array(r['fit'], dtype=np.int64)\n",
    "        }\n",
    "\n",
    "datasets = OrderedDict()\n",
    "splits = ['train', 'valid']\n",
    "datasets['train'] =  ModCloth(\"data/modcloth_final_data_processed_train.csv\")\n",
    "datasets['valid'] =  ModCloth(\"data/modcloth_final_data_processed_valid.csv\")\n",
    "datasets['test'] = ModCloth(\"data/modcloth_final_data_processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0vxnymEpFnRD"
   },
   "outputs": [],
   "source": [
    "# macro - Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
    "# weighted - Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "def compute_metrics(target, pred_probs, averaging = \"macro\"):\n",
    "\n",
    "    pred_labels = pred_probs.argmax(-1)\n",
    "    precision = metrics.precision_score(target, pred_labels, average=averaging)\n",
    "    recall = metrics.recall_score(target, pred_labels, average=averaging)\n",
    "    f1_score = metrics.f1_score(target, pred_labels, average=averaging)\n",
    "    accuracy = metrics.accuracy_score(target, pred_labels)\n",
    "    auc = metrics.roc_auc_score(target, pred_probs, average=averaging, multi_class=\"ovr\")\n",
    "\n",
    "    return precision, recall, f1_score, accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yicAQXX996RY"
   },
   "outputs": [],
   "source": [
    "class Base(nn.Module):\n",
    "    def __init__(self, user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.user_pathway = user_pathway\n",
    "        self.item_pathway = item_pathway\n",
    "        self.combined_pathway = combined_pathway\n",
    "        self.embedding_dim = embed_dim\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_user_embed, embed_dim, max_norm=1.0 )\n",
    "        self.cup_size_embedding = nn.Embedding(num_cupsize_embed, embed_dim, max_norm=1.0 )\n",
    "        self.item_embedding = nn.Embedding(num_item_embed, embed_dim, max_norm=1.0 )\n",
    "        self.category_embedding = nn.Embedding(num_category_embed, embed_dim, max_norm=1.0 )\n",
    "\n",
    "\n",
    "    def forward(self, batch_input):\n",
    "        # Customer Pathway\n",
    "        user_emb = self.user_embedding(batch_input[\"user_id\"])\n",
    "        cup_size_emb = self.cup_size_embedding(batch_input[\"cup_size\"])\n",
    "        user_representation = torch.cat( [user_emb, cup_size_emb, batch_input[\"user_numeric\"]], dim=-1 )\n",
    "        user_representation = self.user_transform_blocks(user_representation)\n",
    "\n",
    "        # Article Pathway\n",
    "        item_emb = self.item_embedding(batch_input[\"item_id\"])\n",
    "        category_emb = self.category_embedding(batch_input[\"category\"])\n",
    "        item_representation = torch.cat( [item_emb, category_emb, batch_input[\"item_numeric\"]], dim=-1 )\n",
    "        item_representation = self.item_transform_blocks(item_representation)\n",
    "\n",
    "        # Combine the pathways\n",
    "        combined_representation = torch.cat( [user_representation, item_representation], dim=-1 )\n",
    "        combined_representation = self.combined_blocks(combined_representation)\n",
    "\n",
    "        # Output layer of logits\n",
    "        logits = self.hidden2output(combined_representation)\n",
    "        pred_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return logits, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PpthRqIj96RZ"
   },
   "outputs": [],
   "source": [
    "class SFNet(Base):\n",
    "    def __init__(self, user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout):\n",
    "        super().__init__(user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout)\n",
    "\n",
    "        # Customer pathway transformation  ==  user_embedding_dim + cup_size_embedding_dim + num_user_numeric_features\n",
    "        user_features_input_size = 2 * self.embedding_dim + NUM_USER_NUMERIC\n",
    "        self.user_pathway.insert(0, user_features_input_size)\n",
    "        self.user_transform_blocks = []\n",
    "        for i in range(1, len(self.user_pathway)):\n",
    "            self.user_transform_blocks.append( SkipBlock( self.user_pathway[i - 1], self.user_pathway[i] ) )\n",
    "            self.user_transform_blocks.append(nn.Dropout(DROPOUT))\n",
    "        self.user_transform_blocks = nn.Sequential(*self.user_transform_blocks)\n",
    "\n",
    "        # Article pathway transformation == item_embedding_dim + category_embedding_dim + num_item_numeric_features\n",
    "        item_features_input_size = 2 * self.embedding_dim + NUM_ITEM_NUMERIC\n",
    "        self.item_pathway.insert(0, item_features_input_size)\n",
    "        self.item_transform_blocks = []\n",
    "        for i in range(1, len(self.item_pathway)):\n",
    "            self.item_transform_blocks.append( SkipBlock( self.item_pathway[i - 1], self.item_pathway[i]) )\n",
    "            self.item_transform_blocks.append(nn.Dropout(DROPOUT))\n",
    "        self.item_transform_blocks = nn.Sequential(*self.item_transform_blocks)\n",
    "\n",
    "        # Combined top layer pathway\n",
    "        # u = output dim of user_transform_blocks, # t = output dim of item_transform_blocks\n",
    "        # Pathway combination through [u, t] # Hence, input dimension will be 2*dim(u)\n",
    "        combined_layer_input_size = 2 * self.user_pathway[-1]\n",
    "        self.combined_pathway.insert(0, combined_layer_input_size)\n",
    "        self.combined_blocks = []\n",
    "        for i in range(1, len(self.combined_pathway)):\n",
    "            self.combined_blocks.append( SkipBlock( self.combined_pathway[i - 1], self.combined_pathway[i]) )\n",
    "            self.combined_blocks.append(nn.Dropout(DROPOUT))\n",
    "        self.combined_blocks = nn.Sequential(*self.combined_blocks)\n",
    "\n",
    "        # Linear transformation from last hidden layer to output\n",
    "        self.hidden2output = nn.Linear(self.combined_pathway[-1], NUM_TARGETS)\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Skip Connection for feed-forward  - ResNet Block \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(input_dim, output_dim)\n",
    "        self.W2 = nn.Linear(output_dim, output_dim)\n",
    "        self.I = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"  z = ReLU(   W2( ReLU( W1(x))) + Projection(x))    \"\"\"\n",
    "        z = relu(self.W2(relu(self.W1(x))) + self.I(x))\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nP3EpC5EFnRE"
   },
   "outputs": [],
   "source": [
    "class MLP(Base):\n",
    "    def __init__(self,user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout):\n",
    "        super().__init__(user_pathway, item_pathway, combined_pathway, embed_dim, num_item_embed, num_user_embed, num_cupsize_embed, num_category_embed, dropout)\n",
    "\n",
    "        # Customer pathway transformation  ==  user_embedding_dim + cup_size_embedding_dim + num_user_numeric_features\n",
    "        user_features_input_size = 2 * self.embedding_dim + NUM_USER_NUMERIC\n",
    "        self.user_pathway.insert(0, user_features_input_size)\n",
    "        self.user_transform_blocks = []\n",
    "        for i in range(1, len(self.user_pathway)):\n",
    "            self.user_transform_blocks.append( LinearBlock( self.user_pathway[i - 1], self.user_pathway[i] ) )\n",
    "        self.user_transform_blocks = nn.Sequential(*self.user_transform_blocks)\n",
    "\n",
    "        # Article pathway transformation == item_embedding_dim + category_embedding_dim + num_item_numeric_features\n",
    "        item_features_input_size = 2 * self.embedding_dim + NUM_ITEM_NUMERIC\n",
    "        self.item_pathway.insert(0, item_features_input_size)\n",
    "        self.item_transform_blocks = []\n",
    "        for i in range(1, len(self.item_pathway)):\n",
    "            self.item_transform_blocks.append( LinearBlock( self.item_pathway[i - 1], self.item_pathway[i])  )\n",
    "        self.item_transform_blocks = nn.Sequential(*self.item_transform_blocks)\n",
    "\n",
    "        # Combined top layer pathway\n",
    "        # u = output dim of user_transform_blocks, # t = output dim of item_transform_blocks\n",
    "        # Pathway combination through [u, t] # Hence, input dimension will be 4*dim(u)\n",
    "        combined_layer_input_size = 2 * self.user_pathway[-1]\n",
    "        self.combined_pathway.insert(0, combined_layer_input_size)\n",
    "        self.combined_blocks = []\n",
    "        for i in range(1, len(self.combined_pathway)):\n",
    "            self.combined_blocks.append( LinearBlock( self.combined_pathway[i - 1], self.combined_pathway[i]) )\n",
    "        self.combined_blocks = nn.Sequential(*self.combined_blocks)\n",
    "\n",
    "        # Linear transformation from last hidden layer to output\n",
    "        self.hidden2output = nn.Linear(self.combined_pathway[-1], NUM_TARGETS)\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Skip Connection for feed-forward  - ResNet Block \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"  z = ReLU(   W2( ReLU( W1(x))) + Projection(x))    \"\"\"\n",
    "        return relu(self.W1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6QlmtdwFnRK",
    "outputId": "97a6415f-1198-408e-b2d8-ed5759920704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MLP(\n",
      "  (user_embedding): Embedding(47958, 40, max_norm=1.0)\n",
      "  (cup_size_embedding): Embedding(12, 40, max_norm=1.0)\n",
      "  (item_embedding): Embedding(1378, 40, max_norm=1.0)\n",
      "  (category_embedding): Embedding(7, 40, max_norm=1.0)\n",
      "  (user_transform_blocks): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (W1): Linear(in_features=85, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (item_transform_blocks): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (W1): Linear(in_features=82, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (combined_blocks): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (3): LinearBlock(\n",
      "      (W1): Linear(in_features=64, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (hidden2output): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Number of model parameters: 2175035\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP(USER_PATHWAY, ITEM_PATHWAY, COMBINED_PATHWAY, EMBED_DIM, NUM_ITEM_EMBED, NUM_USER_EMBED, NUM_CUPSIZE_EMBED, NUM_CATEGORY_EMBED, DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(model)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {total_params}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR, weight_decay= WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SN2h7vipFnRL",
    "outputId": "8ff9253e-ceb9-4206-eddb-3136fd607205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 0/518, Loss=1.13\n",
      "TRAIN Batch Stats 100/518, Loss=0.79\n",
      "TRAIN Batch Stats 200/518, Loss=0.81\n",
      "TRAIN Batch Stats 300/518, Loss=0.79\n",
      "TRAIN Batch Stats 400/518, Loss=0.88\n",
      "TRAIN Batch Stats 500/518, Loss=0.81\n",
      "TRAIN Batch Stats 517/518, Loss=0.93\n",
      "TRAIN Epoch 1 / 2, Mean Total Loss 0.8135095834732056\n",
      "VALID Batch Stats 0/65, Loss=0.69\n",
      "VALID Batch Stats 64/65, Loss=0.80\n",
      "VALID Epoch 1 / 2, Mean Total Loss 0.7953639030456543\n",
      "TRAIN Batch Stats 0/518, Loss=0.77\n",
      "TRAIN Batch Stats 100/518, Loss=0.69\n",
      "TRAIN Batch Stats 200/518, Loss=0.68\n",
      "TRAIN Batch Stats 300/518, Loss=0.78\n",
      "TRAIN Batch Stats 400/518, Loss=0.69\n",
      "TRAIN Batch Stats 500/518, Loss=0.75\n",
      "TRAIN Batch Stats 517/518, Loss=0.82\n",
      "TRAIN Epoch 1 / 2, Mean Total Loss 0.7806781530380249\n",
      "VALID Batch Stats 0/65, Loss=0.67\n",
      "VALID Batch Stats 64/65, Loss=0.73\n",
      "VALID Epoch 1 / 2, Mean Total Loss 0.7573681473731995\n",
      "TRAIN Batch Stats 0/518, Loss=0.72\n",
      "TRAIN Batch Stats 100/518, Loss=0.62\n",
      "TRAIN Batch Stats 200/518, Loss=0.73\n",
      "TRAIN Batch Stats 300/518, Loss=0.75\n",
      "TRAIN Batch Stats 400/518, Loss=0.71\n",
      "TRAIN Batch Stats 500/518, Loss=0.78\n",
      "TRAIN Batch Stats 517/518, Loss=0.66\n",
      "TRAIN Epoch 1 / 2, Mean Total Loss 0.7439345121383667\n",
      "VALID Batch Stats 0/65, Loss=0.59\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 1 / 2, Mean Total Loss 0.7371777296066284\n",
      "TRAIN Batch Stats 0/518, Loss=0.63\n",
      "TRAIN Batch Stats 100/518, Loss=0.77\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.81\n",
      "TRAIN Batch Stats 500/518, Loss=0.69\n",
      "TRAIN Batch Stats 517/518, Loss=0.74\n",
      "TRAIN Epoch 2 / 2, Mean Total Loss 0.7257672548294067\n",
      "VALID Batch Stats 0/65, Loss=0.61\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 2 / 2, Mean Total Loss 0.7379416227340698\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.57\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.68\n",
      "TRAIN Batch Stats 400/518, Loss=0.67\n",
      "TRAIN Batch Stats 500/518, Loss=0.77\n",
      "TRAIN Batch Stats 517/518, Loss=0.72\n",
      "TRAIN Epoch 2 / 2, Mean Total Loss 0.685267984867096\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 2 / 2, Mean Total Loss 0.7522837519645691\n",
      "TRAIN Batch Stats 0/518, Loss=0.68\n",
      "TRAIN Batch Stats 100/518, Loss=0.54\n",
      "TRAIN Batch Stats 200/518, Loss=0.56\n",
      "TRAIN Batch Stats 300/518, Loss=0.50\n",
      "TRAIN Batch Stats 400/518, Loss=0.59\n",
      "TRAIN Batch Stats 500/518, Loss=0.62\n",
      "TRAIN Batch Stats 517/518, Loss=0.50\n",
      "TRAIN Epoch 2 / 2, Mean Total Loss 0.556090235710144\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 2 / 2, Mean Total Loss 0.7838857173919678\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for d in datasets:\n",
    "        for split in splits:\n",
    "            data_loader = DataLoader( dataset=datasets[split], batch_size=BATCH_SIZE, shuffle = (split == \"train\") )\n",
    "\n",
    "            loss_tracker = defaultdict(tensor)\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                target_tracker = []\n",
    "                pred_tracker = []\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits, pred_probs = model(batch)\n",
    "\n",
    "                # loss calculation\n",
    "                loss = loss_criterion(logits, batch[\"fit\"])   # batch['fit'] are the true labels\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                # bookkeepeing\n",
    "                loss_tracker[\"Total Loss\"] = torch.cat((loss_tracker[\"Total Loss\"], loss.view(1)))\n",
    "\n",
    "                if iteration % 100 == 0 or iteration + 1 == len(data_loader):\n",
    "                    print(f\"{split.upper()} Batch Stats {iteration}/{len(data_loader)}, Loss={loss.item() :.2f}\")\n",
    "\n",
    "                if split == \"valid\":\n",
    "                    target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "                    pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "            print( f\"{split.upper()} Epoch {epoch + 1} / {EPOCHS}, Mean Total Loss {torch.mean(loss_tracker['Total Loss'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VopBWrV5FnRM",
    "outputId": "f3856a2d-9d39-41b7-b0aa-6158df08ba8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data ...\n",
      "Evaluating model on test data ...\n",
      "--------------------------------------------------\n",
      "Metrics:\n",
      " Precision = 0.6192578682183549\n",
      " Recall = 0.6494140625\n",
      " F1-score = 0.6293734155069196\n",
      " Accuracy = 0.6494140625\n",
      " AUC = 0.6799020238170111\n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "target_tracker = []\n",
    "pred_tracker = []\n",
    "\n",
    "print(\"Preparing test data ...\")\n",
    "\n",
    "data_loader = DataLoader(dataset = datasets['test'], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Evaluating model on test data ...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, pred_probs = model(batch)\n",
    "\n",
    "        target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "        pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "target_tracker = np.stack(target_tracker[:-1]).reshape(-1)\n",
    "pred_tracker = np.stack(pred_tracker[:-1], axis=0).reshape(-1, NUM_TARGETS)\n",
    "precision, recall, f1_score, accuracy, auc = compute_metrics(target_tracker, pred_tracker, averaging = \"weighted\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metrics:\\n Precision = {precision}\\n Recall = {recall}\\n F1-score = {f1_score}\\n Accuracy = {accuracy}\\n AUC = {auc}\\n \")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVl5-Yfs96Rc",
    "outputId": "086fb05e-46a4-439c-d5ab-1fb51398dab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "SFNet(\n",
      "  (user_embedding): Embedding(47958, 40, max_norm=1.0)\n",
      "  (cup_size_embedding): Embedding(12, 40, max_norm=1.0)\n",
      "  (item_embedding): Embedding(1378, 40, max_norm=1.0)\n",
      "  (category_embedding): Embedding(7, 40, max_norm=1.0)\n",
      "  (user_transform_blocks): Sequential(\n",
      "    (0): SkipBlock(\n",
      "      (W1): Linear(in_features=85, out_features=85, bias=True)\n",
      "      (W2): Linear(in_features=85, out_features=85, bias=True)\n",
      "      (I): Linear(in_features=85, out_features=85, bias=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): SkipBlock(\n",
      "      (W1): Linear(in_features=85, out_features=256, bias=True)\n",
      "      (W2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (I): Linear(in_features=85, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): SkipBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (W2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (item_transform_blocks): Sequential(\n",
      "    (0): SkipBlock(\n",
      "      (W1): Linear(in_features=82, out_features=82, bias=True)\n",
      "      (W2): Linear(in_features=82, out_features=82, bias=True)\n",
      "      (I): Linear(in_features=82, out_features=82, bias=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): SkipBlock(\n",
      "      (W1): Linear(in_features=82, out_features=256, bias=True)\n",
      "      (W2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (I): Linear(in_features=82, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): SkipBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (W2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (combined_blocks): Sequential(\n",
      "    (0): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (W2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): SkipBlock(\n",
      "      (W1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (I): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): SkipBlock(\n",
      "      (W1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (W2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (I): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): SkipBlock(\n",
      "      (W1): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (W2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (I): Linear(in_features=64, out_features=16, bias=True)\n",
      "    )\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (hidden2output): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Number of model parameters: 2727367\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model = SFNet(USER_PATHWAY, ITEM_PATHWAY, COMBINED_PATHWAY, EMBED_DIM, NUM_ITEM_EMBED, NUM_USER_EMBED, NUM_CUPSIZE_EMBED, NUM_CATEGORY_EMBED, DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(model)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {total_params}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR, weight_decay= WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gon_ggoD96Rd",
    "outputId": "a8ed87e8-b9cc-4ced-d8e8-7b6dd4761aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 0/518, Loss=1.14\n",
      "TRAIN Batch Stats 100/518, Loss=0.81\n",
      "TRAIN Batch Stats 200/518, Loss=0.89\n",
      "TRAIN Batch Stats 300/518, Loss=0.87\n",
      "TRAIN Batch Stats 400/518, Loss=0.91\n",
      "TRAIN Batch Stats 500/518, Loss=0.83\n",
      "TRAIN Batch Stats 517/518, Loss=0.74\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.8394917845726013\n",
      "VALID Batch Stats 0/65, Loss=0.72\n",
      "VALID Batch Stats 64/65, Loss=0.82\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.8025875687599182\n",
      "TRAIN Batch Stats 0/518, Loss=0.86\n",
      "TRAIN Batch Stats 100/518, Loss=0.90\n",
      "TRAIN Batch Stats 200/518, Loss=0.89\n",
      "TRAIN Batch Stats 300/518, Loss=0.70\n",
      "TRAIN Batch Stats 400/518, Loss=0.85\n",
      "TRAIN Batch Stats 500/518, Loss=0.77\n",
      "TRAIN Batch Stats 517/518, Loss=0.71\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.8068475127220154\n",
      "VALID Batch Stats 0/65, Loss=0.69\n",
      "VALID Batch Stats 64/65, Loss=0.78\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.7898355722427368\n",
      "TRAIN Batch Stats 0/518, Loss=0.76\n",
      "TRAIN Batch Stats 100/518, Loss=0.93\n",
      "TRAIN Batch Stats 200/518, Loss=0.72\n",
      "TRAIN Batch Stats 300/518, Loss=0.73\n",
      "TRAIN Batch Stats 400/518, Loss=0.83\n",
      "TRAIN Batch Stats 500/518, Loss=0.77\n",
      "TRAIN Batch Stats 517/518, Loss=0.77\n",
      "TRAIN Epoch 1 / 20, Mean Total Loss 0.7991629838943481\n",
      "VALID Batch Stats 0/65, Loss=0.71\n",
      "VALID Batch Stats 64/65, Loss=0.78\n",
      "VALID Epoch 1 / 20, Mean Total Loss 0.7873106598854065\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.79\n",
      "TRAIN Batch Stats 200/518, Loss=0.80\n",
      "TRAIN Batch Stats 300/518, Loss=0.78\n",
      "TRAIN Batch Stats 400/518, Loss=0.76\n",
      "TRAIN Batch Stats 500/518, Loss=0.91\n",
      "TRAIN Batch Stats 517/518, Loss=0.72\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.7888102531433105\n",
      "VALID Batch Stats 0/65, Loss=0.68\n",
      "VALID Batch Stats 64/65, Loss=0.76\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.7749438285827637\n",
      "TRAIN Batch Stats 0/518, Loss=0.78\n",
      "TRAIN Batch Stats 100/518, Loss=0.79\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.85\n",
      "TRAIN Batch Stats 400/518, Loss=0.79\n",
      "TRAIN Batch Stats 500/518, Loss=0.77\n",
      "TRAIN Batch Stats 517/518, Loss=0.66\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.7811945080757141\n",
      "VALID Batch Stats 0/65, Loss=0.67\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.7683157324790955\n",
      "TRAIN Batch Stats 0/518, Loss=0.76\n",
      "TRAIN Batch Stats 100/518, Loss=0.75\n",
      "TRAIN Batch Stats 200/518, Loss=0.75\n",
      "TRAIN Batch Stats 300/518, Loss=0.80\n",
      "TRAIN Batch Stats 400/518, Loss=0.81\n",
      "TRAIN Batch Stats 500/518, Loss=0.76\n",
      "TRAIN Batch Stats 517/518, Loss=0.79\n",
      "TRAIN Epoch 2 / 20, Mean Total Loss 0.7754963636398315\n",
      "VALID Batch Stats 0/65, Loss=0.67\n",
      "VALID Batch Stats 64/65, Loss=0.76\n",
      "VALID Epoch 2 / 20, Mean Total Loss 0.766884446144104\n",
      "TRAIN Batch Stats 0/518, Loss=0.68\n",
      "TRAIN Batch Stats 100/518, Loss=0.89\n",
      "TRAIN Batch Stats 200/518, Loss=0.80\n",
      "TRAIN Batch Stats 300/518, Loss=0.67\n",
      "TRAIN Batch Stats 400/518, Loss=0.71\n",
      "TRAIN Batch Stats 500/518, Loss=0.82\n",
      "TRAIN Batch Stats 517/518, Loss=0.84\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.7682151794433594\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.7591963410377502\n",
      "TRAIN Batch Stats 0/518, Loss=0.80\n",
      "TRAIN Batch Stats 100/518, Loss=0.81\n",
      "TRAIN Batch Stats 200/518, Loss=0.63\n",
      "TRAIN Batch Stats 300/518, Loss=0.76\n",
      "TRAIN Batch Stats 400/518, Loss=0.74\n",
      "TRAIN Batch Stats 500/518, Loss=0.72\n",
      "TRAIN Batch Stats 517/518, Loss=0.88\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.7623715996742249\n",
      "VALID Batch Stats 0/65, Loss=0.66\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.7611445784568787\n",
      "TRAIN Batch Stats 0/518, Loss=0.74\n",
      "TRAIN Batch Stats 100/518, Loss=0.73\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.75\n",
      "TRAIN Batch Stats 400/518, Loss=0.79\n",
      "TRAIN Batch Stats 500/518, Loss=0.72\n",
      "TRAIN Batch Stats 517/518, Loss=0.78\n",
      "TRAIN Epoch 3 / 20, Mean Total Loss 0.7570003271102905\n",
      "VALID Batch Stats 0/65, Loss=0.66\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 3 / 20, Mean Total Loss 0.7555899620056152\n",
      "TRAIN Batch Stats 0/518, Loss=0.64\n",
      "TRAIN Batch Stats 100/518, Loss=0.64\n",
      "TRAIN Batch Stats 200/518, Loss=0.87\n",
      "TRAIN Batch Stats 300/518, Loss=0.68\n",
      "TRAIN Batch Stats 400/518, Loss=0.66\n",
      "TRAIN Batch Stats 500/518, Loss=0.78\n",
      "TRAIN Batch Stats 517/518, Loss=0.80\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.7551410794258118\n",
      "VALID Batch Stats 0/65, Loss=0.67\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.7574425339698792\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.77\n",
      "TRAIN Batch Stats 200/518, Loss=0.88\n",
      "TRAIN Batch Stats 300/518, Loss=0.79\n",
      "TRAIN Batch Stats 400/518, Loss=0.76\n",
      "TRAIN Batch Stats 500/518, Loss=0.80\n",
      "TRAIN Batch Stats 517/518, Loss=0.73\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.7509867548942566\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.7538021206855774\n",
      "TRAIN Batch Stats 0/518, Loss=0.68\n",
      "TRAIN Batch Stats 100/518, Loss=0.78\n",
      "TRAIN Batch Stats 200/518, Loss=0.77\n",
      "TRAIN Batch Stats 300/518, Loss=0.83\n",
      "TRAIN Batch Stats 400/518, Loss=0.73\n",
      "TRAIN Batch Stats 500/518, Loss=0.76\n",
      "TRAIN Batch Stats 517/518, Loss=0.93\n",
      "TRAIN Epoch 4 / 20, Mean Total Loss 0.750490128993988\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.76\n",
      "VALID Epoch 4 / 20, Mean Total Loss 0.7525163888931274\n",
      "TRAIN Batch Stats 0/518, Loss=0.76\n",
      "TRAIN Batch Stats 100/518, Loss=0.88\n",
      "TRAIN Batch Stats 200/518, Loss=0.78\n",
      "TRAIN Batch Stats 300/518, Loss=0.81\n",
      "TRAIN Batch Stats 400/518, Loss=0.80\n",
      "TRAIN Batch Stats 500/518, Loss=0.73\n",
      "TRAIN Batch Stats 517/518, Loss=0.78\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.7471047043800354\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.7509312629699707\n",
      "TRAIN Batch Stats 0/518, Loss=0.72\n",
      "TRAIN Batch Stats 100/518, Loss=0.73\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.69\n",
      "TRAIN Batch Stats 400/518, Loss=0.79\n",
      "TRAIN Batch Stats 500/518, Loss=0.66\n",
      "TRAIN Batch Stats 517/518, Loss=0.75\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.7447822690010071\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.7479597330093384\n",
      "TRAIN Batch Stats 0/518, Loss=0.71\n",
      "TRAIN Batch Stats 100/518, Loss=0.75\n",
      "TRAIN Batch Stats 200/518, Loss=0.85\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.80\n",
      "TRAIN Batch Stats 500/518, Loss=0.87\n",
      "TRAIN Batch Stats 517/518, Loss=0.72\n",
      "TRAIN Epoch 5 / 20, Mean Total Loss 0.7436333298683167\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 5 / 20, Mean Total Loss 0.7474603652954102\n",
      "TRAIN Batch Stats 0/518, Loss=0.70\n",
      "TRAIN Batch Stats 100/518, Loss=0.72\n",
      "TRAIN Batch Stats 200/518, Loss=0.75\n",
      "TRAIN Batch Stats 300/518, Loss=0.74\n",
      "TRAIN Batch Stats 400/518, Loss=0.87\n",
      "TRAIN Batch Stats 500/518, Loss=0.65\n",
      "TRAIN Batch Stats 517/518, Loss=0.69\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.7418166995048523\n",
      "VALID Batch Stats 0/65, Loss=0.67\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.7509227991104126\n",
      "TRAIN Batch Stats 0/518, Loss=0.82\n",
      "TRAIN Batch Stats 100/518, Loss=0.81\n",
      "TRAIN Batch Stats 200/518, Loss=0.68\n",
      "TRAIN Batch Stats 300/518, Loss=0.81\n",
      "TRAIN Batch Stats 400/518, Loss=0.76\n",
      "TRAIN Batch Stats 500/518, Loss=0.81\n",
      "TRAIN Batch Stats 517/518, Loss=0.81\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.7400254607200623\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.744806706905365\n",
      "TRAIN Batch Stats 0/518, Loss=0.63\n",
      "TRAIN Batch Stats 100/518, Loss=0.74\n",
      "TRAIN Batch Stats 200/518, Loss=0.79\n",
      "TRAIN Batch Stats 300/518, Loss=0.72\n",
      "TRAIN Batch Stats 400/518, Loss=0.65\n",
      "TRAIN Batch Stats 500/518, Loss=0.66\n",
      "TRAIN Batch Stats 517/518, Loss=0.90\n",
      "TRAIN Epoch 6 / 20, Mean Total Loss 0.7389783263206482\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 6 / 20, Mean Total Loss 0.7451237440109253\n",
      "TRAIN Batch Stats 0/518, Loss=0.59\n",
      "TRAIN Batch Stats 100/518, Loss=0.68\n",
      "TRAIN Batch Stats 200/518, Loss=0.76\n",
      "TRAIN Batch Stats 300/518, Loss=0.79\n",
      "TRAIN Batch Stats 400/518, Loss=0.79\n",
      "TRAIN Batch Stats 500/518, Loss=0.59\n",
      "TRAIN Batch Stats 517/518, Loss=0.77\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.7370293140411377\n",
      "VALID Batch Stats 0/65, Loss=0.66\n",
      "VALID Batch Stats 64/65, Loss=0.68\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.7478946447372437\n",
      "TRAIN Batch Stats 0/518, Loss=0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 100/518, Loss=0.81\n",
      "TRAIN Batch Stats 200/518, Loss=0.61\n",
      "TRAIN Batch Stats 300/518, Loss=0.61\n",
      "TRAIN Batch Stats 400/518, Loss=0.68\n",
      "TRAIN Batch Stats 500/518, Loss=0.83\n",
      "TRAIN Batch Stats 517/518, Loss=0.76\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.7369799017906189\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.7454690933227539\n",
      "TRAIN Batch Stats 0/518, Loss=0.71\n",
      "TRAIN Batch Stats 100/518, Loss=0.89\n",
      "TRAIN Batch Stats 200/518, Loss=0.86\n",
      "TRAIN Batch Stats 300/518, Loss=0.73\n",
      "TRAIN Batch Stats 400/518, Loss=0.75\n",
      "TRAIN Batch Stats 500/518, Loss=0.80\n",
      "TRAIN Batch Stats 517/518, Loss=0.76\n",
      "TRAIN Epoch 7 / 20, Mean Total Loss 0.7360489368438721\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 7 / 20, Mean Total Loss 0.7514167428016663\n",
      "TRAIN Batch Stats 0/518, Loss=0.61\n",
      "TRAIN Batch Stats 100/518, Loss=0.73\n",
      "TRAIN Batch Stats 200/518, Loss=0.77\n",
      "TRAIN Batch Stats 300/518, Loss=0.69\n",
      "TRAIN Batch Stats 400/518, Loss=0.72\n",
      "TRAIN Batch Stats 500/518, Loss=0.69\n",
      "TRAIN Batch Stats 517/518, Loss=0.81\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.7357243895530701\n",
      "VALID Batch Stats 0/65, Loss=0.66\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.7468863725662231\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.74\n",
      "TRAIN Batch Stats 200/518, Loss=0.73\n",
      "TRAIN Batch Stats 300/518, Loss=0.68\n",
      "TRAIN Batch Stats 400/518, Loss=0.74\n",
      "TRAIN Batch Stats 500/518, Loss=0.80\n",
      "TRAIN Batch Stats 517/518, Loss=0.78\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.7365685105323792\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.68\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.7473667860031128\n",
      "TRAIN Batch Stats 0/518, Loss=0.65\n",
      "TRAIN Batch Stats 100/518, Loss=0.80\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.84\n",
      "TRAIN Batch Stats 500/518, Loss=0.72\n",
      "TRAIN Batch Stats 517/518, Loss=0.79\n",
      "TRAIN Epoch 8 / 20, Mean Total Loss 0.736403226852417\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 8 / 20, Mean Total Loss 0.7471229434013367\n",
      "TRAIN Batch Stats 0/518, Loss=0.75\n",
      "TRAIN Batch Stats 100/518, Loss=0.69\n",
      "TRAIN Batch Stats 200/518, Loss=0.77\n",
      "TRAIN Batch Stats 300/518, Loss=0.70\n",
      "TRAIN Batch Stats 400/518, Loss=0.69\n",
      "TRAIN Batch Stats 500/518, Loss=0.65\n",
      "TRAIN Batch Stats 517/518, Loss=0.79\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.7332608699798584\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.7435713410377502\n",
      "TRAIN Batch Stats 0/518, Loss=0.70\n",
      "TRAIN Batch Stats 100/518, Loss=0.75\n",
      "TRAIN Batch Stats 200/518, Loss=0.70\n",
      "TRAIN Batch Stats 300/518, Loss=0.75\n",
      "TRAIN Batch Stats 400/518, Loss=0.83\n",
      "TRAIN Batch Stats 500/518, Loss=0.72\n",
      "TRAIN Batch Stats 517/518, Loss=0.79\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.7334019541740417\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.68\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.7439045906066895\n",
      "TRAIN Batch Stats 0/518, Loss=0.72\n",
      "TRAIN Batch Stats 100/518, Loss=0.79\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.74\n",
      "TRAIN Batch Stats 400/518, Loss=0.80\n",
      "TRAIN Batch Stats 500/518, Loss=0.74\n",
      "TRAIN Batch Stats 517/518, Loss=0.63\n",
      "TRAIN Epoch 9 / 20, Mean Total Loss 0.7312895655632019\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 9 / 20, Mean Total Loss 0.743071436882019\n",
      "TRAIN Batch Stats 0/518, Loss=0.72\n",
      "TRAIN Batch Stats 100/518, Loss=0.76\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.76\n",
      "TRAIN Batch Stats 400/518, Loss=0.73\n",
      "TRAIN Batch Stats 500/518, Loss=0.77\n",
      "TRAIN Batch Stats 517/518, Loss=0.57\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.7327958345413208\n",
      "VALID Batch Stats 0/65, Loss=0.66\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.7531440258026123\n",
      "TRAIN Batch Stats 0/518, Loss=0.71\n",
      "TRAIN Batch Stats 100/518, Loss=0.76\n",
      "TRAIN Batch Stats 200/518, Loss=0.73\n",
      "TRAIN Batch Stats 300/518, Loss=0.68\n",
      "TRAIN Batch Stats 400/518, Loss=0.86\n",
      "TRAIN Batch Stats 500/518, Loss=0.79\n",
      "TRAIN Batch Stats 517/518, Loss=0.67\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.7313466668128967\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.7424020767211914\n",
      "TRAIN Batch Stats 0/518, Loss=0.84\n",
      "TRAIN Batch Stats 100/518, Loss=0.64\n",
      "TRAIN Batch Stats 200/518, Loss=0.81\n",
      "TRAIN Batch Stats 300/518, Loss=0.72\n",
      "TRAIN Batch Stats 400/518, Loss=0.87\n",
      "TRAIN Batch Stats 500/518, Loss=0.74\n",
      "TRAIN Batch Stats 517/518, Loss=0.77\n",
      "TRAIN Epoch 10 / 20, Mean Total Loss 0.7328441143035889\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 10 / 20, Mean Total Loss 0.7441678643226624\n",
      "TRAIN Batch Stats 0/518, Loss=0.63\n",
      "TRAIN Batch Stats 100/518, Loss=0.67\n",
      "TRAIN Batch Stats 200/518, Loss=0.88\n",
      "TRAIN Batch Stats 300/518, Loss=0.71\n",
      "TRAIN Batch Stats 400/518, Loss=0.71\n",
      "TRAIN Batch Stats 500/518, Loss=0.80\n",
      "TRAIN Batch Stats 517/518, Loss=0.83\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.7315554618835449\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.7442673444747925\n",
      "TRAIN Batch Stats 0/518, Loss=0.78\n",
      "TRAIN Batch Stats 100/518, Loss=0.61\n",
      "TRAIN Batch Stats 200/518, Loss=0.67\n",
      "TRAIN Batch Stats 300/518, Loss=0.74\n",
      "TRAIN Batch Stats 400/518, Loss=0.69\n",
      "TRAIN Batch Stats 500/518, Loss=0.73\n",
      "TRAIN Batch Stats 517/518, Loss=0.76\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.7301142811775208\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.7464432716369629\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.62\n",
      "TRAIN Batch Stats 200/518, Loss=0.69\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.85\n",
      "TRAIN Batch Stats 500/518, Loss=0.71\n",
      "TRAIN Batch Stats 517/518, Loss=0.62\n",
      "TRAIN Epoch 11 / 20, Mean Total Loss 0.7282682657241821\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.68\n",
      "VALID Epoch 11 / 20, Mean Total Loss 0.7506766319274902\n",
      "TRAIN Batch Stats 0/518, Loss=0.68\n",
      "TRAIN Batch Stats 100/518, Loss=0.75\n",
      "TRAIN Batch Stats 200/518, Loss=0.80\n",
      "TRAIN Batch Stats 300/518, Loss=0.81\n",
      "TRAIN Batch Stats 400/518, Loss=0.82\n",
      "TRAIN Batch Stats 500/518, Loss=0.79\n",
      "TRAIN Batch Stats 517/518, Loss=0.75\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.729500412940979\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.7475221157073975\n",
      "TRAIN Batch Stats 0/518, Loss=0.78\n",
      "TRAIN Batch Stats 100/518, Loss=0.74\n",
      "TRAIN Batch Stats 200/518, Loss=0.66\n",
      "TRAIN Batch Stats 300/518, Loss=0.80\n",
      "TRAIN Batch Stats 400/518, Loss=0.84\n",
      "TRAIN Batch Stats 500/518, Loss=0.77\n",
      "TRAIN Batch Stats 517/518, Loss=0.68\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.7300239205360413\n",
      "VALID Batch Stats 0/65, Loss=0.62\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.7437481880187988\n",
      "TRAIN Batch Stats 0/518, Loss=0.74\n",
      "TRAIN Batch Stats 100/518, Loss=0.71\n",
      "TRAIN Batch Stats 200/518, Loss=0.76\n",
      "TRAIN Batch Stats 300/518, Loss=0.72\n",
      "TRAIN Batch Stats 400/518, Loss=0.75\n",
      "TRAIN Batch Stats 500/518, Loss=0.79\n",
      "TRAIN Batch Stats 517/518, Loss=0.77\n",
      "TRAIN Epoch 12 / 20, Mean Total Loss 0.729619026184082\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.67\n",
      "VALID Epoch 12 / 20, Mean Total Loss 0.7413309812545776\n",
      "TRAIN Batch Stats 0/518, Loss=0.76\n",
      "TRAIN Batch Stats 100/518, Loss=0.70\n",
      "TRAIN Batch Stats 200/518, Loss=0.85\n",
      "TRAIN Batch Stats 300/518, Loss=0.73\n",
      "TRAIN Batch Stats 400/518, Loss=0.73\n",
      "TRAIN Batch Stats 500/518, Loss=0.65\n",
      "TRAIN Batch Stats 517/518, Loss=0.82\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.7270897626876831\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.743092954158783\n",
      "TRAIN Batch Stats 0/518, Loss=0.77\n",
      "TRAIN Batch Stats 100/518, Loss=0.71\n",
      "TRAIN Batch Stats 200/518, Loss=0.73\n",
      "TRAIN Batch Stats 300/518, Loss=0.61\n",
      "TRAIN Batch Stats 400/518, Loss=0.80\n",
      "TRAIN Batch Stats 500/518, Loss=0.70\n",
      "TRAIN Batch Stats 517/518, Loss=0.50\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.7251643538475037\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.7413802146911621\n",
      "TRAIN Batch Stats 0/518, Loss=0.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 100/518, Loss=0.82\n",
      "TRAIN Batch Stats 200/518, Loss=0.68\n",
      "TRAIN Batch Stats 300/518, Loss=0.78\n",
      "TRAIN Batch Stats 400/518, Loss=0.83\n",
      "TRAIN Batch Stats 500/518, Loss=0.71\n",
      "TRAIN Batch Stats 517/518, Loss=0.82\n",
      "TRAIN Epoch 13 / 20, Mean Total Loss 0.7251079678535461\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 13 / 20, Mean Total Loss 0.7411651611328125\n",
      "TRAIN Batch Stats 0/518, Loss=0.94\n",
      "TRAIN Batch Stats 100/518, Loss=0.64\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.73\n",
      "TRAIN Batch Stats 400/518, Loss=0.75\n",
      "TRAIN Batch Stats 500/518, Loss=0.70\n",
      "TRAIN Batch Stats 517/518, Loss=0.62\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.7257499694824219\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.7454534769058228\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.74\n",
      "TRAIN Batch Stats 200/518, Loss=0.71\n",
      "TRAIN Batch Stats 300/518, Loss=0.71\n",
      "TRAIN Batch Stats 400/518, Loss=0.75\n",
      "TRAIN Batch Stats 500/518, Loss=0.85\n",
      "TRAIN Batch Stats 517/518, Loss=0.61\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.7256861329078674\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.7472900748252869\n",
      "TRAIN Batch Stats 0/518, Loss=0.80\n",
      "TRAIN Batch Stats 100/518, Loss=0.75\n",
      "TRAIN Batch Stats 200/518, Loss=0.79\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.71\n",
      "TRAIN Batch Stats 500/518, Loss=0.71\n",
      "TRAIN Batch Stats 517/518, Loss=0.69\n",
      "TRAIN Epoch 14 / 20, Mean Total Loss 0.7251291275024414\n",
      "VALID Batch Stats 0/65, Loss=0.62\n",
      "VALID Batch Stats 64/65, Loss=0.74\n",
      "VALID Epoch 14 / 20, Mean Total Loss 0.746196985244751\n",
      "TRAIN Batch Stats 0/518, Loss=0.70\n",
      "TRAIN Batch Stats 100/518, Loss=0.61\n",
      "TRAIN Batch Stats 200/518, Loss=0.84\n",
      "TRAIN Batch Stats 300/518, Loss=0.58\n",
      "TRAIN Batch Stats 400/518, Loss=0.70\n",
      "TRAIN Batch Stats 500/518, Loss=0.83\n",
      "TRAIN Batch Stats 517/518, Loss=0.66\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.7248845100402832\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.7429825663566589\n",
      "TRAIN Batch Stats 0/518, Loss=0.77\n",
      "TRAIN Batch Stats 100/518, Loss=0.77\n",
      "TRAIN Batch Stats 200/518, Loss=0.62\n",
      "TRAIN Batch Stats 300/518, Loss=0.75\n",
      "TRAIN Batch Stats 400/518, Loss=0.69\n",
      "TRAIN Batch Stats 500/518, Loss=0.59\n",
      "TRAIN Batch Stats 517/518, Loss=0.61\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.7242687344551086\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.7500450611114502\n",
      "TRAIN Batch Stats 0/518, Loss=0.77\n",
      "TRAIN Batch Stats 100/518, Loss=0.70\n",
      "TRAIN Batch Stats 200/518, Loss=0.67\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.77\n",
      "TRAIN Batch Stats 500/518, Loss=0.80\n",
      "TRAIN Batch Stats 517/518, Loss=0.92\n",
      "TRAIN Epoch 15 / 20, Mean Total Loss 0.7239305377006531\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 15 / 20, Mean Total Loss 0.7405893206596375\n",
      "TRAIN Batch Stats 0/518, Loss=0.58\n",
      "TRAIN Batch Stats 100/518, Loss=0.66\n",
      "TRAIN Batch Stats 200/518, Loss=0.70\n",
      "TRAIN Batch Stats 300/518, Loss=0.81\n",
      "TRAIN Batch Stats 400/518, Loss=0.78\n",
      "TRAIN Batch Stats 500/518, Loss=0.65\n",
      "TRAIN Batch Stats 517/518, Loss=0.71\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.7236217260360718\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.7447840571403503\n",
      "TRAIN Batch Stats 0/518, Loss=0.67\n",
      "TRAIN Batch Stats 100/518, Loss=0.71\n",
      "TRAIN Batch Stats 200/518, Loss=0.86\n",
      "TRAIN Batch Stats 300/518, Loss=0.75\n",
      "TRAIN Batch Stats 400/518, Loss=0.69\n",
      "TRAIN Batch Stats 500/518, Loss=0.68\n",
      "TRAIN Batch Stats 517/518, Loss=0.73\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.7234426140785217\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.7429395318031311\n",
      "TRAIN Batch Stats 0/518, Loss=0.76\n",
      "TRAIN Batch Stats 100/518, Loss=0.69\n",
      "TRAIN Batch Stats 200/518, Loss=0.83\n",
      "TRAIN Batch Stats 300/518, Loss=0.78\n",
      "TRAIN Batch Stats 400/518, Loss=0.70\n",
      "TRAIN Batch Stats 500/518, Loss=0.73\n",
      "TRAIN Batch Stats 517/518, Loss=0.76\n",
      "TRAIN Epoch 16 / 20, Mean Total Loss 0.721542477607727\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.73\n",
      "VALID Epoch 16 / 20, Mean Total Loss 0.7405576109886169\n",
      "TRAIN Batch Stats 0/518, Loss=0.69\n",
      "TRAIN Batch Stats 100/518, Loss=0.57\n",
      "TRAIN Batch Stats 200/518, Loss=0.73\n",
      "TRAIN Batch Stats 300/518, Loss=0.74\n",
      "TRAIN Batch Stats 400/518, Loss=0.71\n",
      "TRAIN Batch Stats 500/518, Loss=0.80\n",
      "TRAIN Batch Stats 517/518, Loss=0.74\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.721680223941803\n",
      "VALID Batch Stats 0/65, Loss=0.66\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.7442946434020996\n",
      "TRAIN Batch Stats 0/518, Loss=0.67\n",
      "TRAIN Batch Stats 100/518, Loss=0.69\n",
      "TRAIN Batch Stats 200/518, Loss=0.64\n",
      "TRAIN Batch Stats 300/518, Loss=0.70\n",
      "TRAIN Batch Stats 400/518, Loss=0.78\n",
      "TRAIN Batch Stats 500/518, Loss=0.74\n",
      "TRAIN Batch Stats 517/518, Loss=0.61\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.7202069759368896\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.7460131049156189\n",
      "TRAIN Batch Stats 0/518, Loss=0.72\n",
      "TRAIN Batch Stats 100/518, Loss=0.87\n",
      "TRAIN Batch Stats 200/518, Loss=0.72\n",
      "TRAIN Batch Stats 300/518, Loss=0.64\n",
      "TRAIN Batch Stats 400/518, Loss=0.76\n",
      "TRAIN Batch Stats 500/518, Loss=0.69\n",
      "TRAIN Batch Stats 517/518, Loss=0.71\n",
      "TRAIN Epoch 17 / 20, Mean Total Loss 0.721203625202179\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.73\n",
      "VALID Epoch 17 / 20, Mean Total Loss 0.7466785311698914\n",
      "TRAIN Batch Stats 0/518, Loss=0.60\n",
      "TRAIN Batch Stats 100/518, Loss=0.71\n",
      "TRAIN Batch Stats 200/518, Loss=0.78\n",
      "TRAIN Batch Stats 300/518, Loss=0.80\n",
      "TRAIN Batch Stats 400/518, Loss=0.64\n",
      "TRAIN Batch Stats 500/518, Loss=0.68\n",
      "TRAIN Batch Stats 517/518, Loss=0.63\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.7201256155967712\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.744509756565094\n",
      "TRAIN Batch Stats 0/518, Loss=0.73\n",
      "TRAIN Batch Stats 100/518, Loss=0.67\n",
      "TRAIN Batch Stats 200/518, Loss=0.72\n",
      "TRAIN Batch Stats 300/518, Loss=0.70\n",
      "TRAIN Batch Stats 400/518, Loss=0.73\n",
      "TRAIN Batch Stats 500/518, Loss=0.67\n",
      "TRAIN Batch Stats 517/518, Loss=0.64\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.721177875995636\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.69\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.7439413070678711\n",
      "TRAIN Batch Stats 0/518, Loss=0.77\n",
      "TRAIN Batch Stats 100/518, Loss=0.86\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.74\n",
      "TRAIN Batch Stats 400/518, Loss=0.79\n",
      "TRAIN Batch Stats 500/518, Loss=0.75\n",
      "TRAIN Batch Stats 517/518, Loss=0.80\n",
      "TRAIN Epoch 18 / 20, Mean Total Loss 0.7198153734207153\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 18 / 20, Mean Total Loss 0.7457046508789062\n",
      "TRAIN Batch Stats 0/518, Loss=0.72\n",
      "TRAIN Batch Stats 100/518, Loss=0.77\n",
      "TRAIN Batch Stats 200/518, Loss=0.68\n",
      "TRAIN Batch Stats 300/518, Loss=0.83\n",
      "TRAIN Batch Stats 400/518, Loss=0.63\n",
      "TRAIN Batch Stats 500/518, Loss=0.70\n",
      "TRAIN Batch Stats 517/518, Loss=0.80\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.7184702754020691\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.7460392117500305\n",
      "TRAIN Batch Stats 0/518, Loss=0.73\n",
      "TRAIN Batch Stats 100/518, Loss=0.72\n",
      "TRAIN Batch Stats 200/518, Loss=0.68\n",
      "TRAIN Batch Stats 300/518, Loss=0.77\n",
      "TRAIN Batch Stats 400/518, Loss=0.73\n",
      "TRAIN Batch Stats 500/518, Loss=0.75\n",
      "TRAIN Batch Stats 517/518, Loss=0.71\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.7190579771995544\n",
      "VALID Batch Stats 0/65, Loss=0.65\n",
      "VALID Batch Stats 64/65, Loss=0.72\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.7459359765052795\n",
      "TRAIN Batch Stats 0/518, Loss=0.78\n",
      "TRAIN Batch Stats 100/518, Loss=0.92\n",
      "TRAIN Batch Stats 200/518, Loss=0.64\n",
      "TRAIN Batch Stats 300/518, Loss=0.70\n",
      "TRAIN Batch Stats 400/518, Loss=0.86\n",
      "TRAIN Batch Stats 500/518, Loss=0.68\n",
      "TRAIN Batch Stats 517/518, Loss=0.85\n",
      "TRAIN Epoch 19 / 20, Mean Total Loss 0.720796525478363\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.73\n",
      "VALID Epoch 19 / 20, Mean Total Loss 0.7494685053825378\n",
      "TRAIN Batch Stats 0/518, Loss=0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch Stats 100/518, Loss=0.74\n",
      "TRAIN Batch Stats 200/518, Loss=0.74\n",
      "TRAIN Batch Stats 300/518, Loss=0.64\n",
      "TRAIN Batch Stats 400/518, Loss=0.72\n",
      "TRAIN Batch Stats 500/518, Loss=0.74\n",
      "TRAIN Batch Stats 517/518, Loss=0.71\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.7205126285552979\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.71\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.7478137612342834\n",
      "TRAIN Batch Stats 0/518, Loss=0.79\n",
      "TRAIN Batch Stats 100/518, Loss=0.78\n",
      "TRAIN Batch Stats 200/518, Loss=0.67\n",
      "TRAIN Batch Stats 300/518, Loss=0.82\n",
      "TRAIN Batch Stats 400/518, Loss=0.64\n",
      "TRAIN Batch Stats 500/518, Loss=0.73\n",
      "TRAIN Batch Stats 517/518, Loss=0.78\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.7174954414367676\n",
      "VALID Batch Stats 0/65, Loss=0.63\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.7478765845298767\n",
      "TRAIN Batch Stats 0/518, Loss=0.62\n",
      "TRAIN Batch Stats 100/518, Loss=0.69\n",
      "TRAIN Batch Stats 200/518, Loss=0.77\n",
      "TRAIN Batch Stats 300/518, Loss=0.74\n",
      "TRAIN Batch Stats 400/518, Loss=0.68\n",
      "TRAIN Batch Stats 500/518, Loss=0.85\n",
      "TRAIN Batch Stats 517/518, Loss=0.63\n",
      "TRAIN Epoch 20 / 20, Mean Total Loss 0.7175785303115845\n",
      "VALID Batch Stats 0/65, Loss=0.64\n",
      "VALID Batch Stats 64/65, Loss=0.70\n",
      "VALID Epoch 20 / 20, Mean Total Loss 0.7518382668495178\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for d in datasets:\n",
    "        for split in splits:\n",
    "            data_loader = DataLoader( dataset=datasets[split], batch_size=BATCH_SIZE, shuffle = (split == \"train\") )\n",
    "\n",
    "            loss_tracker = defaultdict(tensor)\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                target_tracker = []\n",
    "                pred_tracker = []\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits, pred_probs = model(batch)\n",
    "\n",
    "                # loss calculation\n",
    "                loss = loss_criterion(logits, batch[\"fit\"])   # batch['fit'] are the true labels\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                # bookkeepeing\n",
    "                loss_tracker[\"Total Loss\"] = torch.cat((loss_tracker[\"Total Loss\"], loss.view(1)))\n",
    "\n",
    "                if iteration % 100 == 0 or iteration + 1 == len(data_loader):\n",
    "                    print(f\"{split.upper()} Batch Stats {iteration}/{len(data_loader)}, Loss={loss.item() :.2f}\")\n",
    "\n",
    "                if split == \"valid\":\n",
    "                    target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "                    pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "            print( f\"{split.upper()} Epoch {epoch + 1} / {EPOCHS}, Mean Total Loss {torch.mean(loss_tracker['Total Loss'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfZJOYqG96Rd",
    "outputId": "5932e070-1ed1-466b-a1a5-b7274e2386a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data ...\n",
      "Evaluating model on test data ...\n",
      "--------------------------------------------------\n",
      "Metrics:\n",
      " Precision = 0.6396702524325568\n",
      " Recall = 0.688720703125\n",
      " F1-score = 0.5702997811215722\n",
      " Accuracy = 0.688720703125\n",
      " AUC = 0.6984894609825474\n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "target_tracker = []\n",
    "pred_tracker = []\n",
    "\n",
    "print(\"Preparing test data ...\")\n",
    "\n",
    "data_loader = DataLoader(dataset = datasets['test'], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Evaluating model on test data ...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    for iteration, batch in enumerate(data_loader):\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, pred_probs = model(batch)\n",
    "\n",
    "        target_tracker.append(batch[\"fit\"].cpu().numpy())\n",
    "        pred_tracker.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "target_tracker = np.stack(target_tracker[:-1]).reshape(-1)\n",
    "pred_tracker = np.stack(pred_tracker[:-1], axis=0).reshape(-1, NUM_TARGETS)\n",
    "precision, recall, f1_score, accuracy, auc = compute_metrics(target_tracker, pred_tracker, averaging = \"weighted\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metrics:\\n Precision = {precision}\\n Recall = {recall}\\n F1-score = {f1_score}\\n Accuracy = {accuracy}\\n AUC = {auc}\\n \")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1e5d2acd1631045f09d34790d51dde8c5f13a0de3f2ba4add1e385dbc0b204e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
